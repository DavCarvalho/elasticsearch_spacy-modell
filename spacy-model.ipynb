{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download pt_core_news_lg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk\n",
    "%pip install spacy-wordnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendendo sobre synset e lemmas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\drc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\drc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O | o | o\n",
      "jogador | jogador | jog\n",
      "de | de | de\n",
      "futebol | futebol | futebol\n",
      "chutou | chutar | chut\n",
      "a | o | a\n",
      "bola | bola | bol\n",
      "para | para | par\n",
      "o | o | o\n",
      "gol | gol | gol\n",
      ". | . | .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('rslp')\n",
    "\n",
    "# Carregue o modelo SpaCy\n",
    "nlp = spacy.load('pt_core_news_lg')\n",
    "\n",
    "# Adicione o WordnetAnnotator ao pipeline de processamento\n",
    "nlp.add_pipe('spacy_wordnet', after='lemmatizer')\n",
    "\n",
    "\n",
    "doc = nlp(\"O jogador de futebol chutou a bola para o gol.\")\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, \"|\", token.lemma_,\"|\", stemmer.stem(token.text))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\drc\\AppData\\Local\\anaconda3\\envs\\py38\\lib\\runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\drc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "c:\\Users\\drc\\AppData\\Local\\anaconda3\\envs\\py38\\lib\\runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package omw to\n",
      "[nltk_data]     C:\\Users\\drc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python -m nltk.downloader wordnet\n",
    "!python -m nltk.downloader omw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy-wordnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendendo sobre o space_wordnet\n",
    "\n",
    "spaCy Wordnet is a simple custom component for using WordNet, MultiWordnet and WordNet domains with spaCy.\n",
    "\n",
    "Get all synsets for a processed token. For example, getting all the synsets (word senses) of the word bank.\n",
    "Get and filter synsets by domain. For example, getting synonyms of the verb withdraw in the financial domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eu vou (cozinhar|preparar) (marmêndoa|batata|batatinha) fritas .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator \n",
    "\n",
    "\n",
    "nlp = spacy.load('pt_core_news_lg')\n",
    "nlp.add_pipe(\"spacy_wordnet\", after='lemmatizer')\n",
    "\n",
    "\n",
    "text = \"Eu vou cozinhar batatas fritas.\"\n",
    "economy_domains = ['food', 'cooking']\n",
    "enriched_sentence = []\n",
    "sentence = nlp(text)\n",
    "\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, \"|\", token.lemma_,\"|\", stemmer.stem(token.text))\n",
    "\n",
    "\n",
    "for token in sentence:\n",
    "    # We get those synsets within the desired domains\n",
    "    synsets = token._.wordnet.wordnet_synsets_for_domain(economy_domains)\n",
    "    if not synsets:\n",
    "        enriched_sentence.append(token.text)\n",
    "    else:\n",
    "        lemmas_for_synset = [lemma for s in synsets for lemma in s.lemma_names('por')]\n",
    "        # If we found a synset in the economy domains\n",
    "        # we get the variants and add them to the enriched sentence\n",
    "        enriched_sentence.append('({})'.format('|'.join(set(lemmas_for_synset))))\n",
    "\n",
    "# Let's see our enriched sentence\n",
    "print(' '.join(enriched_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformando os pdfs em texto em uma arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "folder_path = \"./pdf\"\n",
    "output_folder = \"./output_text\"\n",
    "\n",
    "# Certifique-se de que o diretório de saída exista, se não existir, crie-o\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Para cada arquivo em folder_path\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Verificar se termina com .pdf\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        # Pegar o path do arquivo\n",
    "        pdf_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Criar um contexto para garantir que o arquivo será fechado\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            # Criar um leitor de PDF\n",
    "            pdf = PdfReader(f)\n",
    "\n",
    "            # Inicializar o texto do PDF\n",
    "            pdf_text = \"\"\n",
    "\n",
    "            # Iterar sobre as páginas e concatenar o texto\n",
    "            for page in pdf.pages:\n",
    "                pdf_text += page.extract_text()\n",
    "\n",
    "            # Criar o nome do arquivo de saída\n",
    "            output_file_name = f\"{filename}_completo.txt\"\n",
    "            output_file_path = os.path.join(output_folder, output_file_name)\n",
    "\n",
    "            # Escrever o texto completo em um arquivo de saída\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(pdf_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tentando fazer sinonimos para cada token dos pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\drc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "import os  # Para interagir com o sistema operacional\n",
    "import spacy  # Para processamento de linguagem natural\n",
    "import nltk  # Toolkit de processamento de linguagem natural\n",
    "from nltk.corpus import wordnet  # Corpus do WordNet da NLTK, blibioteca que contem palavras \n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator  # Anotador WordNet para o SpaCy\n",
    "\n",
    "#Adiciona anotações relacionadas ao WordNet ao pipeline do SpaCy. No código, é usado para obter informações do WordNet para os tokens do SpaCy.\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Caminho para os pdf em formato de texto\n",
    "folder_path = './output_text/'\n",
    "# Caminho para o arquivo de saída\n",
    "output_file_path = 'sinonimos_output.txt'\n",
    "\n",
    "# Carregar o modelo SpaCy\n",
    "nlp = spacy.load('pt_core_news_lg')\n",
    "# Adicionar o Anotador Wordnet ao pipeline do SpaCy\n",
    "nlp.add_pipe(\"spacy_wordnet\", after='lemmatizer')\n",
    "\n",
    "# Inicializar o stemmer da NLTK\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "# Domínios para procurar sinônimos\n",
    "engineering_domains = ['engineering', 'isometrics', 'projects', 'structural_engineering', 'civil_engineering', 'cities',\n",
    "                       'mechanical_engineering', 'electrical_engineering', 'computer_engineering', 'software_engineering', 'systems_engineering', \n",
    "                       'materials_science', 'physics', 'chemistry', 'mathematics', 'biology', 'geology', 'chemical_engineering', 'environmental_engineering', \n",
    "                       'construction', 'manufacturing', 'project_management', 'quality_management', 'safety_engineering', 'risk_management', 'technology_management', \n",
    "                       'innovation_management', 'design', 'research', 'development', 'consulting', 'education', 'training', 'certification', 'licensing', 'regulation', \n",
    "                       'compliance', 'standards', 'quality', 'safety', 'health', 'security', 'environment', 'sustainability', 'sustainable_engineering', 'green_engineering', \n",
    "                       'renewable_energy', 'energy_efficiency', 'smart_cities', 'smart_grid', 'internet_of_things', 'machine_learning', 'artificial_intelligence', 'data_science', \n",
    "                       'analytics', 'statistics', 'modeling', 'simulation', 'testing', 'validation', 'verification', 'requirements', 'specifications', 'design_engineering', \n",
    "                       'process_engineering', 'product_engineering', 'system_engineering', 'project_engineering', 'engineering_management', 'engineering_education', \n",
    "                       'engineering_consulting', 'engineering_research', 'engineering_development', 'engineering_training', 'engineering_certification', \n",
    "                       'engineering_licensing', 'engineering_regulation', 'engineering_compliance', 'engineering_standards', 'engineering_quality', 'engineering_safety', \n",
    "                       'engineering_health', 'engineering_security', 'engineering_environment', 'engineering_sustainability', 'engineering_sustainable_engineering', \n",
    "                       'engineering_green_engineering', 'engineering_renewable_energy', 'engineering_energy_efficiency', 'engineering_smart_cities', 'engineering_smart_grid', 'engineering_internet_of_things', 'engineering_machine_learning', 'engineering_artificial_intelligence', 'engineering_data_science', 'engineering_analytics', 'engineering_statistics', 'engineering_modeling', 'engineering_simulation', 'engineering_testing', 'engineering_validation', 'engineering_verification', 'engineering_requirements', 'engineering_specifications', 'engineering_design', 'engineering_process', 'engineering_product', 'engineering_system', 'engineering_project', 'engineering_management', 'engineering_education', 'engineering_consulting', 'engineering_research', 'engineering_development', 'engineering_training', 'engineering_certification', 'engineering_licensing', 'engineering_regulation', 'engineering_compliance', 'engineering_standards', 'engineering_quality', 'engineering_safety', 'engineering_health', 'engineering_security', 'engineering_environment', 'engineering_sustainability', 'engineering_sustainable_engineering', 'engineering_green_engineering', 'engineering_renewable_energy', 'engineering_energy_efficiency', 'engineering_smart_cities', 'engineering_smart_grid', 'engineering_internet_of_things', 'engineering_machine_learning', 'engineering_artificial_intelligence', 'engineering_data_science', 'engineering_analytics', 'engineering_statistics', 'engineering_modeling', 'engineering_simulation', 'engineering_testing', 'engineering_validation', 'engineering_verification', 'engineering_requirements', \n",
    "                       'engineering_specifications', 'engineering_design', 'engineering_process', 'engineering_product', 'engineering_system', 'engineering_project', 'engineering_management', 'engineering_education', 'engineering_consulting', 'engineering_research', 'engineering_development', 'engineering_training', 'engineering_certification', 'engineering_licensing', 'engineering_regulation', 'engineering_compliance', 'engineering_standards', 'engineering_quality', 'engineering_safety', 'engineering_health', 'engineering_security', 'engineering_environment', 'engineering_sustainability', 'engineering_sustainable_engineering', 'engineering_green_engineering', 'engineering_renewable_energy', 'engineering_energy_efficiency', 'engineering_smart_cities', 'engineering_smart_grid', 'engineering_internet_of_things', 'engineering_machine_learning', 'engineering_artificial_intelligence', 'engineering_data_science', 'engineering_analytics', 'engineering_statistics', 'engineering_modeling', 'engineering_simulation', 'engineering_testing', 'engineering_validation', 'engineering_verification', 'engineering_requirements', 'engineering_specifications', 'engineering_design', 'engineering_process', 'engineering_product', 'engineering_system', 'engineering_project', 'engineering_management', 'engineering_education', 'engineering_consulting', 'engineering_research', 'engineering_development', 'engineering_training', 'engineering_certification', 'engineering_licensing', 'engineering_regulation', 'engineering_compliance', 'engineering_standards', 'engineering_quality', 'engineering_safety', 'engineering_health', 'engineering_security', 'engineering_environment', 'engineering_sustainability', 'engineering_sustainable_engineering', 'engineering_green_engineering', 'engineering_renewable_energy', 'engineering_energy_efficiency', 'engineering_smart_cities']\n",
    "\n",
    "\n",
    "# Lista para armazenar os resultados\n",
    "resultados = []\n",
    "\n",
    "# Função para obter sinônimos de uma palavra\n",
    "def get_synonyms(word, pos=wordnet.NOUN):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word, pos=pos):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "# Iterar sobre os arquivos no diretório de saída\n",
    "for filename in os.listdir(folder_path):\n",
    "     # Ler o conteúdo do arquivo PDF\n",
    "    doc = nlp(open(os.path.join(folder_path, filename), encoding=\"utf-8\").read())\n",
    "\n",
    "    # Lista para armazenar os resultados para cada documento\n",
    "    document_result = []\n",
    "\n",
    "     # Iterar sobre os tokens no documento\n",
    "    for token in doc:\n",
    "        # Obter synsets para o token nos domínios de engenharia\n",
    "        synsets = token._.wordnet.wordnet_synsets_for_domain(engineering_domains)\n",
    "        \n",
    "        # Lista para armazenar os lemmas e sinônimos para cada token\n",
    "        token_result = []\n",
    "\n",
    "         # Se não houver synsets, adicione o texto original\n",
    "        if not synsets:\n",
    "            token_result.append(token.text)\n",
    "        else:\n",
    "             # Obter lemmas para cada synset\n",
    "            lemmas_for_synset = [lemma for s in synsets for lemma in s.lemma_names('por')]\n",
    "            # Obter sinônimos para cada lemma\n",
    "            synonyms_for_synset = set()\n",
    "            for synset in synsets:\n",
    "                for lemma in synset.lemma_names('por'):\n",
    "                    synonyms_for_synset.update(get_synonyms(lemma, pos=wordnet.NOUN))\n",
    "            # Adicionar lemmas e sinônimos ao resultado\n",
    "            token_result.append('({})'.format('|'.join(set(lemmas_for_synset + list(synonyms_for_synset)))))\n",
    "        \n",
    "        # Adicionar informações a resultados para cada token\n",
    "        document_result.append(f'{token.text} | {token.lemma_} | {stemmer.stem(token.text)} | {\" \".join(token_result)}')\n",
    "\n",
    "    # Adicionar resultados para o documento atual à lista geral\n",
    "    resultados.extend(document_result)\n",
    "\n",
    "# Escrever os resultados em um novo arquivo\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write('\\n'.join(resultados))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\drc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "import os  # Para interagir com o sistema operacional\n",
    "import spacy  # Para processamento de linguagem natural\n",
    "import nltk  # Toolkit de processamento de linguagem natural\n",
    "from nltk.corpus import wordnet  # Corpus do WordNet da NLTK, blibioteca que contem palavras \n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator  # Anotador WordNet para o SpaCy\n",
    "\n",
    "#Adiciona anotações relacionadas ao WordNet ao pipeline do SpaCy. No código, é usado para obter informações do WordNet para os tokens do SpaCy.\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Caminho para os pdf em formato de texto\n",
    "folder_path = './output_text/'\n",
    "# Caminho para o arquivo de saída\n",
    "output_file_path = 'sinonimos_output.txt'\n",
    "\n",
    "# Carregar o modelo SpaCy\n",
    "nlp = spacy.load('pt_core_news_lg')\n",
    "# Adicionar o Anotador Wordnet ao pipeline do SpaCy\n",
    "nlp.add_pipe(\"spacy_wordnet\", after='lemmatizer')\n",
    "\n",
    "# Inicializar o stemmer da NLTK\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "\n",
    "# Lista para armazenar os resultados\n",
    "resultados = []\n",
    "\n",
    "# Função para obter sinônimos de uma palavra\n",
    "def get_synonyms(word, pos=wordnet.NOUN):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word, pos=pos):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "# Iterar sobre os arquivos no diretório de saída\n",
    "for filename in os.listdir(folder_path):\n",
    "     # Ler o conteúdo do arquivo PDF\n",
    "    doc = nlp(open(os.path.join(folder_path, filename), encoding=\"utf-8\").read())\n",
    "\n",
    "    # Lista para armazenar os resultados para cada documento\n",
    "    document_result = []\n",
    "\n",
    "     # Iterar sobre os tokens no documento\n",
    "    for token in doc:\n",
    "\n",
    "        # Obter todos os synsets para o token\n",
    "        synsets = token._.wordnet.synsets()\n",
    "        \n",
    "        # Lista para armazenar os lemmas e sinônimos para cada token\n",
    "        token_result = []\n",
    "\n",
    "         # Se não houver synsets, adicione o texto original\n",
    "        if not synsets:\n",
    "            token_result.append(token.text)\n",
    "        else:\n",
    "             # Obter lemmas para cada synset\n",
    "            lemmas_for_synset = [lemma for s in synsets for lemma in s.lemma_names('por')]\n",
    "            # Obter sinônimos para cada lemma\n",
    "            synonyms_for_synset = set()\n",
    "            for synset in synsets:\n",
    "                for lemma in synset.lemma_names('por'):\n",
    "                    synonyms_for_synset.update(get_synonyms(lemma, pos=wordnet.NOUN))\n",
    "            # Adicionar lemmas e sinônimos ao resultado\n",
    "            token_result.append('({})'.format('|'.join(set(lemmas_for_synset + list(synonyms_for_synset)))))\n",
    "        \n",
    "        # Adicionar informações a resultados para cada token\n",
    "        document_result.append(f'{token.text} | {token.lemma_} | {stemmer.stem(token.text)} | {\" \".join(token_result)}')\n",
    "        \n",
    "\n",
    "    # Adicionar resultados para o documento atual à lista geral\n",
    "    resultados.extend(document_result)\n",
    "\n",
    "# Escrever os resultados em um novo arquivo\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write('\\n'.join(resultados))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\drc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to\n",
      "[nltk_data]     C:\\Users\\drc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOGADOR []\n",
      "DE []\n",
      "FUTEBOL []\n"
     ]
    }
   ],
   "source": [
    "import os  # Para interagir com o sistema operacional\n",
    "import spacy  # Para processamento de linguagem natural\n",
    "import nltk  # Toolkit de processamento de linguagem natural\n",
    "\n",
    "#classes do spacy que relacioandos ao wordnet que é uma coleção de palavras \n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator \n",
    "from spacy.language import Language\n",
    "\n",
    "#o wordnet é uma coleção de sinônimos e antônimos de palavras, enquanto o corpus OWM é uma versão multilíngue do WordNet \n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw')\n",
    "\n",
    "#carrega o modelo pre-treinado do spacy \n",
    "nlp = spacy.load('pt_core_news_lg')\n",
    "\n",
    "#A classe Language é uma classe de alto nível do SpaCy que representa o idioma e seu modelo. \n",
    "#Você pode adicionar processadores personalizados ao pipeline de processamento de linguagem natural do SpaCy usando a classe\n",
    "@Language.component(\"wordnet_annotator\")\n",
    "def add_wordnet_annotator(doc):\n",
    "    wordnet = WordnetAnnotator(nlp, \"wordnet\")\n",
    "    doc = wordnet(doc)\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"wordnet_annotator\", name=\"wordnet\", last=True)\n",
    "\n",
    "\n",
    "def has_synonyms(token):\n",
    "    return len(token._.synonyms) > 0\n",
    "\n",
    "def synonyms(token, synonyms):\n",
    "    token._.synonyms = synonyms\n",
    "\n",
    "spacy.tokens.Token.set_extension(\"has_synonyms\", getter=has_synonyms, force=True)\n",
    "spacy.tokens.Token.set_extension(\"set_sinonimos\", method=synonyms, force=True)\n",
    "spacy.tokens.Token.set_extension(\"synonyms\", default=[], force=True)\n",
    "\n",
    "folder_path = \"./output_text/\"\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    with open(os.path.join(folder_path, filename), encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "        doc = nlp(text)\n",
    "\n",
    "        infos_subs = []\n",
    "        substantivos = set()\n",
    "\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"NOUN\" and token.text not in substantivos and token.text and token.text.isalpha():\n",
    "                \n",
    "                if token._.wordnet is not None:\n",
    "                    print(token.text)\n",
    "                    synsets = token._.wordnet.synsets()\n",
    "                    print(\"sysnets: \", synsets)\n",
    "                    lemmas = [lemma for s in synsets for lemma in s.lemma_names('por')]\n",
    "                    print(\"lemmas: \", lemmas_for_synset)\n",
    "                    synonyms = [lemma.name() for lemma in lemmas]\n",
    "                    print(\"synonyms: \", synonyms)\n",
    "                    token._.set_sinonimos(synonyms)\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_windows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
